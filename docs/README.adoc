= OpenShift-Ansible Integration Lab

== 0 Introduction

=== 0.1 Usecase

Today we are building a Widget inventory tracking system. It's comprised of a simple data driven application backed by
MySQL. We will start with how to deploy and configure the MySQL server via Ansible, follow that up with how to manage
the application's OpenShift deployment with Ansible, and finally bring the database automation into the OpenShift
Service Catalog.

=== 0.2 Ansible and OpenShift

The goal of this lab is to show how the
https://docs.openshift.com/container-platform/latest/getting_started/index.html[OpenShift Container Platform]
and https://www.ansible.com/resources/get-started[Ansible Automation] can build on each other to increase innovation
and accelerate delivery.

First we'll introduce how Ansible can be used to automate provisioning and configuration of application dependencies.
Then we'll look at how we can leverage Ansible for application configuration within OpenShift as part of a pipeline.
Finally, we will dig into how the
https://docs.openshift.com/container-platform/3.11/architecture/service_catalog/ansible_service_broker.html[OpenShift
Ansible Broker] adds self-service automation to the platform.

The source code for this lab is on https://github.com/srang/rh-openshift-ansible-broker-lab[GitHub], but we will be
doing most of our work in AWS so don't worry about cloning the repo yet. One other thing to note is that the source code
will be available well after the lab (although not the lab environment itself) and will work mostly out of the box on
the Container Development Kit (it's where the lab was developed) and I'll likely publish a `minishift` branch that works
completely out of the box. Indicate interest in a minishift version of the lab by starring the repo and opening an
issue in GitHub.

*NOTE*: this demo was built for OpenShift 3.10. When 4.0 releases, a new version of this lab will likely need to be released

== 1 Install MySQL Server with Ansible Engine

=== 1.1 Review Playbook

First you'll need to connect to your lab. To do that you'll need to ssh using your student account to the Master Node
for your student account. The command will look like this:

```
$ ssh -o 'StrictHostKeyChecking no' student1@ec2-18-234-37-92.compute-1.amazonaws.com <- your Master Node
```

Alternatively you can skip the `StrictHostKeyChecking` option and just accept host key check prompt.
Once on the Master Node, you'll need to set up shop. First we're gonna install `git` so we can pull down the lab source
code.

```
$ sudo yum install -y git
```

Next we're gonna clone the lab:

```
$ git clone https://github.com/srang/rh-openshift-ansible-broker-lab.git
```

Now for convenience let's define an environment variable on our Master Node that says where the source code is:

```
$ echo export GIT_BASE="$(pwd)/rh-openshift-ansible-broker-lab" >> ~/.bashrc
$ source ~/.bashrc
$ cd $GIT_BASE
```

Finally, we need to make sure we're looking at the right branch of the lab (we don't want to see the answers) so run:

```
$ git checkout master
```

Go into the `database-provision-playbook` directory and look around. This is a good opportunity to install your favorite
terminal-based editor (mine is and will always be `vim`). Adventurous developers could also fork the lab repository,
clone it locally (to your personal machine), edit code in an IDE, and update git remotes to pull from a fork but for the
purpose of this lab we will assume we're working directly on the Master Node.

=== 1.2 Provision MySQL server on VM

Most of this playbook is pre-configured for you, take a look at `database-playbook.yml`. It is installing MariaDB (an
opensource fork of MySQL), configuring a database on the server, and adding a user for that database. Let's try running
it!

In order to run this playbook in your environment, you'll need to update the `hosts` file in the inventory folder.
The username needs to match *your* student account or you will get authentication errors.

```
[all:vars]
ansible_user=student1 <- This is the line that needs to be changed
```

Additionally, you'll need to update `hosts` with the hosts in *your* lab. Update the `[db-server]` section with your
database node hostname.

```
[db-server]
ec2-54-161-113-184.compute-1.amazonaws.com <- This should match your environments Database Node
```

Once you've made these updates, you're ready to run the playbook (it may take a couple minutes on the 'install
mysql-server' step):

```
$ ansible-playbook -vv database-playbook.yml -i inventory/
```

Let's connect to the database and see what's there

```
$ sudo yum install -y mariadb
$ mysql --host=ec2-54-161-113-184.compute-1.amazonaws.com --port=3306 --user=widget --password=widget01 widgettest
MariaDB [widgettest]> show tables;
```

We should see that our database is _completely_ empty. But we can fix that pretty easily. We're going to jump ahead a
little bit and "deploy" our widget factory in our workspace and connect to the database there.

=== 1.3 Run Canary Application

In order to build and run our application, we're going to need some tools:

```
$ cd $GIT_BASE/widget-factory
$ sudo yum install -y rh-maven35 --enablerepo=rhel-server-rhscl-7-rpms
$ scl enable rh-maven35 bash
```

Now we can run the app, but we need to add the database connection information. Open the `application-canary.yaml` file
in the `src/main/resources` folder, and update the jdbc url to have the correct host info:

```
spring:
  jpa.hibernate.ddl-auto: create
  datasource:
    url: jdbc:mysql://ec2-18-234-37-92.compute-1.amazonaws.com:3306/widgettest <- this is the line needs updated hostname
```

With that done we can build and run the application on our deployable canary profile:

```
$ mvn clean install -Popenshift
$ SPRING_PROFILES_ACTIVE=canary java -jar target/widget-factory.jar &
```

This will start the job in the background (so it will log to `stdout` but you can still type commands, it's easiest if
wait for the boot logs to finish). Now lets try hitting the service a couple times to see what's been created, then
we can verify in the database that everything persists correctly.

```
$ curl localhost:8080/widgets
$ curl -H 'Content-type: application/json' -d '{"label": "NEW01", "version": "V1", "description": "some new thing"}' localhost:8080/widgets
$ curl localhost:8080/widgets
```

Now we can stop the application and dive into the db:

```
$ kill %1
$ mysql --host=ec2-54-161-113-184.compute-1.amazonaws.com --port=3306 --user=widget --password=widget01 widgettest
MariaDB [widgettest]> show tables;
MariaDB [widgettest]> select * from widget;
MariaDB [widgettest]> exit
```

Now we should see a lot more (relatively) stuff in our database! So we can all stop and go home right? Wrong, there is a
lot more we can do to add stability, flexibility and security to our stack.

== 2 Deploy Application

We've been running and testing things manually. Deploying to OpenShift adds stability, ensuring uptime and
scalability, and by defining a CI/CD pipeline, we standardize how the application is built and deployed. To start, let's
create an OpenShift project. Make sure when logging in you are using the web console url for *your* lab:

```
$ oc login --insecure-skip-tls-verify=true https://ec2-18-234-37-92.compute-1.amazonaws.com -u admin -p redhat01 <- Ensure to use correct OpenShift cluster
$ oc new-project widget-factory
```

=== 2.1 Auto-deploy Jenkins

A sample pipeline has already been defined for you in `widget-factory/Jenkinsfile`. One of the nice things about
OpenShift is how it integrates with Jenkins for CI/CD. By defining a pipeline build configuration, OpenShift will
automatically deploy Jenkins -- more information on the mechanism behind this can be found in the
https://docs.openshift.com/container-platform/3.10/install_config/configuring_pipeline_execution.html["Configuring
pipeline execution"] docs. Alternatively we could proactively deploy Jenkins using the Template Service Broker and the
OpenShift Service Catalog (more on these in later sections).

=== 2.2 Configure `widget-jenkins-agent`

Before we can run our application pipeline we actually need to build a brand new Jenkins agent image. We need this for
tooling around our deployment playbook (explained in following sections).

```
$ cd $GIT_BASE/widget-jenkins-agent
$ oc process -f agent-pipeline.yml --param=SOURCE_REF=master | oc apply -f-
```

We are actually using pipelines to build our agent! It seems a little recursive but the idea of standardizing everything
with automation makes things repeatable and that leads to confidence in frequent deployments (which is awesome). Go into
the web console and watch your Jenkins instance come up, then we'll kick off a build of our `widget-jenkins-agent`.
If you'd rather trigger a pipeline run from the CLI, you can run this command (once Jenkins is healthy):

```
$ oc start-build widget-jenkins-agent-pipeline
```

*Note* it may take a few minutes for Jenkins to finish its post deploy bootstrapping before running the build. Keep an
eye on the Jenkins master pod logs for indication as to whether its completed this bootstrapping.

Your password to Jenkins will be same as your OpenShift password (`admin`:`redhat01`). After this image is built, it
will automatically show up as an available agent in the kubernetes-plugin configuration section in your Jenkins instance
and can be used by specifying the label `widget-jenkins-agent`.

=== 2.3 Review Application

Now let's finally take a look at that widget-factory service:

```
$ cd $GIT_BASE/widget-factory
```

It's a simple-spring data service, one controller is setup as a `spring-data-rest` interface that autoconfigures CRUD
operations on our `widget` object. There is a second controller that exposes a service interface tied to a widget
repository interface allowing for building more custom queries. The important parts of the application (for the purpose
of this lab) are how we are planning to automate building, deploying and connecting the application to our database (for
now `widgettest` configured in Section 1).

=== 2.4 Ansible OpenShift Applier

Let's take a look inside the `.applier` folder, under `templates` you'll see a number of YAML files specifying an
OpenShift template for various resources. As you may expect, `build.yml` specifies how to build and store the image,
while `deploy.yml` specifies how to deploy the application. The `db-service.yml` contains configuration for how to
connect to our database, exposing the external hostname of the server as an OpenShift internal service (more
https://docs.openshift.com/container-platform/3.10/dev_guide/integrating_external_services.html[info]). It also creates
the encoded secret `mysql` that our deployment uses.

This directory is used by an ansible role call the https://github.com/redhat-cop/openshift-applier[`openshift-applier`].
The role allows for template instantiation as an ansible-playbook which makes it easy to inject into a pipeline (which
is what we've done).

If you look at the stages of the pipeline, you'll see mostly standard steps for building the jar, building the image,
and deploying it to the cluster, but there are also some ansible commands in the "Apply OpenShift Manifests with Ansible"
stage. These commands install the role from ansible galaxy and then apply the build, deploy and db configuration to our
namespace.

=== 2.5 Deploy Application

We are now ready to deploy our application (make sure the widget-jenkins-agent image has successfully built first).
Let's create our pipeline, you'll need to make sure to use the correct database hostname:

```
$ oc process -f widget-pipeline.yml --param=SOURCE_REF=master --param=DATABASE_HOST=ec2-54-89-60-203.compute-1.amazonaws.com | oc apply -f-
$ oc start-build widget-factory-pipeline
```

Now go into Jenkins to watch your build continue. When it has completed, you should see it deployed in the web-console
with all the configuration necessary for it to connect to your `widgettest` database. Try using the route in OpenShift
to recreate the NEW widget we created in Section 1 (it gets deleted when the application redeploys as part of the schema
initialization).

It's pretty cool seeing all these pieces come together, but what happens if you need another database? Will you have
someone ssh into the database box each and every time? Should you put the playbook in Ansible Tower? Will your ops team
have to run it for you even though its a relatively low impact development change? Let's see if there is a better way to
handle this.

== 3 Self-service MySQL DB Provisioning

We have automation in place for provisioning a database server and adding a database to it, but we've been running it
manually. Now if only there were a way to bring that automation into the OpenShift Service Catalog for self-service
consumption by the application teams... (hint: that's exactly what we're going to do).

=== 3.1 Automation Service Broker

The https://docs.openshift.com/container-platform/3.10/architecture/service_catalog/ansible_service_broker.html[OpenShift
Ansible Broker] (or http://automationbroker.io/[Automation Broker]) provides a way to deploy playbooks via the OpenShift
Service Catalog. Playbooks are packaged as https://docs.openshift.com/container-platform/3.10/apb_devel/[Ansible Playbook
Bundles] which are lightweight images containing parametrized playbooks. Let's try our hand at converting our
`database-provisioning-playbook` to an APB.

=== 3.2 Build an APB

First off, let's make sure we're in the right place to work on this.

```
$ git checkout my-apb
$ cd $GIT_BASE/database-provision-apb
```

A Jenkinsfile has been provided for you, but only for guidance. Unless you've forked the lab repository, you need to
trigger a https://docs.openshift.com/container-platform/3.10/dev_guide/dev_tutorials/binary_builds.html[Binary Build] for
OpenShift to pick up *your* code and not the upstream lab code. Additionally, we need the build to push to the
`openshift` namespace in order for the Ansible Broker to see the new image. That is a different namespace and will
require us to change up some permissions.

```
$ oc create imagestream database-provision-apb -n openshift
$ oc policy add-role-to-user system:image-builder system:serviceaccount:widget-factory:builder -n openshift
$ oc new-build --binary=true --name=database-provision-apb -n widget-factory
$ oc patch bc/database-provision-apb -p '{"spec": {"output":{"to": {"namespace": "openshift"}}}}' -n widget-factory
```

This creates a binary build that pushes to a new imagestream in the openshift namespace and gives the builder account
in our namespace permission to push to that imagestream.

Next we are going to need a couple more utilities for interacting with the broker:

```
$ sudo yum install -y apb --enablerepo=rhel-7-server-ose-3.11-rpms <- yes the 3.11 rpms (we want latest of apb)
$ apb version <- you have to run this to generate default config (and make sure it is 1.9.7 or higher)
$ sed -i 's/osb/ansible-service-broker/' ~/.apb/defaults.json
$ sed -i 's/openshift-automation-service-broker/openshift-ansible-service-broker/' ~/.apb/defaults # for ocp 3.10 vs minishift
$ oc apply -f broker-config.yml
$ oc rollout latest dc/asb -n openshift-ansible-service-broker
```

Now it's finally time to get our hands dirty. A skeleton APB is already created for you in this directory. The pieces to
note are the `apb.yml` which specifies required parameters, the `Dockerfile` which specifies how to build the image, and
the `playbooks` folder which is how OpenShift will invoke certain "actions" on the APB (these can probably stay as-is).

*Note*: This is where you get to write some code

Use the Jenkinsfile for guidance on how to invoke builds but the basic steps are:

```
$ apb bundle prepare
$ oc start-build --follow --from-dir . database-provision-apb
$ apb broker bootstrap
$ apb broker catalog
```

Your APB should be listed when you run the `apb broker catalog` command. If it doesn't, make sure to check the logs of
the broker (`oc logs --since=30s dc/asb -n openshift-ansible-service-broker`). If your APB is showing up in the `apb
broker catalog` command but not the service catalog, you may need to run `apb catalog relist` (and maybe refresh the
catalog page).

=== 3.3 Provision Database and Credentials

Once your APB is being correctly listed in the `abp broker catalog` command, you're ready to call it from the service
catalog. In the top right corner of the web-console there is an "Add to project" button that allows you to search the
catalog for your APB. You may need to rerun the `apb catalog relist` command to have the catalog reindex.

=== 3.4 Update Application to Use Bindings

As long as your APB creates a secret with the same name (`mysql`) there shouldn't be anything to do here except making
sure that your application pipeline doesn't try to overwrite it (take a look at
`widget-factory/.applier/templates/db-service.yml`).