= OpenShift-Ansible Integration Lab

== 0 Introduction

=== 0.1 Usecase

Today we are building a Widget inventory tracking system. It's comprised of a simple data driven application backed by
MySQL. We will be deploying the application on OpenShift and configuring a database server on a separate RHEL virtual
machine.

=== 0.2 Ansible and OpenShift

The goal of this lab is to show how the
https://docs.openshift.com/container-platform/latest/getting_started/index.html[OpenShift Container Platform]
and https://www.ansible.com/resources/get-started[Ansible Automation] can build on each other to increase innovation
and accelerate delivery.

First we'll introduce how Ansible can be used to automate provisioning and configuration of application dependencies.
Then we'll look at how we can leverage Ansible for application configuration within OpenShift as part of a pipeline.
Finally, we will dig into how the
https://docs.openshift.com/container-platform/3.11/architecture/service_catalog/ansible_service_broker.html[OpenShift Ansible Broker]
adds self-service automation to the platform.

== 1 Install MySQL Server with Ansible Engine

=== 1.1 Introduction to Ansible

=== 1.2 Review Playbook

First you'll need to connect to your lab. To do that you'll need to ssh using your student account to the Master Node
for your student account. The command will look like this:

```
$ ssh -o 'StrictHostKeyChecking no' student1@ec2-18-234-37-92.compute-1.amazonaws.com
```

Alternatively you can skip the `StrictHostKeyChecking` option and just accept host key check prompt.
Once on the Master Node, you'll need to set up shop. First we're gonna install `git` so we can pull down the lab source
code.

```
$ sudo yum install -y git
```

Next we're gonna clone the lab:

```
$ git clone https://github.com/srang/rh-openshift-ansible-broker-lab.git
```

Now for convenience let's define an environment variable on our Master Node that says where the source code is:

```
$ echo export GIT_BASE="$(pwd)/rh-openshift-ansible-broker-lab" >> ~/.bashrc
$ source ~/.bashrc
$ cd $GIT_BASE
```

Finally, we need to make sure we're looking at the right branch of the lab (we're in section 1) so run:

```
$ git checkout section-1
```

Go into the `database-provision-playbook` directory and look around. This is a good opportunity to install your favorite
terminal-based editor. Adventurous developers could also fork the lab repository, clone it locally (to your personal
machine), edit code in an IDE, and update git remotes to pull from a fork but for the purpose of this lab we will assume
we're working directly on the Master Node.

=== 1.3 Provision MySQL server on VM

Most of this playbook is pre-configured for you, take a look at `database-provision.yml`. It is installing MariaDB (an
opensource fork of MySQL), configuring a database on the server, and adding a user for that database. Let's try running
it!

In order to run this playbook in your environment, you'll need to update the `hosts` file in the inventory folder.
The username needs to match *your* student account or you will get authentication errors.

```
[all:vars]
ansible_user=student1 <- This is the line that needs to be changed
```

Additionally, you'll need to update `hosts` with the hosts in *your* lab. Update the `[db-server]` section with your
database node hostname.

```
[db-server]
ec2-54-161-113-184.compute-1.amazonaws.com <- This should match your environments Database Node
```

Once you've made these updates, you're ready to run the playbook (it may take a couple minutes on the 'install
mysql-server' step):

```
$ ansible-playbook -vv database-playbook.yml -i inventory/
```

Let's connect to the database and see what's there

```
$ mysql --user=widget --password=widget01 widgettest
MariaDB [widgettest]> show tables;
```

We should see that our database is _completely_ empty. But we can fix that pretty easily. We're going to jump ahead a
little bit and "deploy" our widget factory in our workspace and connect to the database there.

=== 1.4 Run Canary Application

In order to build and run our application, we're going to need some tools:

```
$ cd $GIT_BASE/widget-factory
$ sudo yum install -y rh-maven35 --enablerepo=rhel-server-rhscl-7-rpms
$ scl enable rh-maven35 bash
```

Now we can run the app, but we need to add the database connection information. Open the `application-canary.yaml` file
in the `src/main/resources` folder, and update the jdbc url to have the correct host info:

```
spring:
  jpa.hibernate.ddl-auto: create
  datasource:
    url: jdbc:mysql://ec2-18-234-37-92.compute-1.amazonaws.com:3306/widgettest <- this is the line needs updated hostname
```

With that done we can build and run the application on our deployable canary profile:

```
$ mvn clean install -Popenshift
$ SPRING_PROFILES_ACTIVE=canary java -jar target/widget-factory.jar &
```

This will start the job in the background (so it will log to `stdout` but you can still type commands, it's easiest if
wait for the boot logs to finish). Now lets try hitting the service a couple times to see what's been created, then
we can verify in the database that everything persists correctly.

```
$ curl localhost:8080/widgets
$ curl -H 'Content-type: application/json' -d '{"label": "NEW01", "version": "V1", "description": "some new thing"}' \
    localhost:8080/widgets
$ curl localhost:8080/widgets
```

Now we can stop the application and dive into the db:

```
$ kill %1
$ mysql --user=widget --password=widget01 widgettest
MariaDB [widgettest]> show tables;
MariaDB [widgettest]> select * from widget;
MariaDB [widgettest]> exit
```

Now we should see a lot more (relatively) stuff in our database! So we can all stop and go home right? Wrong there is a
lot more we can do to add stability, flexibility and security to our stack.

== 2 Deploy Application

We've been running and testing things manually. Deploying to OpenShift adds stability around ensuring uptime and
scalability, and by defining a pipeline we standardize how the application is built and deployed. To start, let's create
an OpenShift project. Make sure when logging in you are using the web console url for *your* lab:

```
$ oc login --insecure-skip-tls-verify=true https://ec2-18-234-37-92.compute-1.amazonaws.com -u admin -p redhat01 <- Ensure to use correct OpenShift cluster
$ oc new-project widget-factory
```

=== 2.1 Auto-deploy Jenkins

A sample pipeline has already been defined for you in `widget-factory/Jenkinsfile`. One of the nice things about
OpenShift is how it integrates with Jenkins for CI/CD. By defining a pipeline build configuration, OpenShift will
automatically deploy Jenkins -- more information on the mechanism behind this can be found in the
https://docs.openshift.com/container-platform/3.10/install_config/configuring_pipeline_execution.html["Configuring pipeline execution"]
docs. Alternatively we could proactively deploy Jenkins using the Template Service Broker and the OpenShift Service
Catalog (more on these in later sections).

=== 2.2 Configure `widget-jenkins-agent`

Before we can run our application pipeline we actually need to build a brand new Jenkins agent image. We need this for
tooling around our deployment playbook (explained in following sections).

```
$ git checkout section-2
$ cd $GIT_BASE/widget-jenkins-agent
$ oc process -f agent-pipeline.yml --param=SOURCE_REF=section-2 | oc apply -f-
```

We are actually using pipelines to build our agent! It seems a little recursive but the idea of standardizing everything
with automation makes things repeatable and that leads to confidence in frequent deployments (which is awesome). Go into
the web console and watch your Jenkins instance come up, then we'll kick off a build of our `widget-jenkins-agent`.
If you'd rather trigger a pipeline run from the CLI, you can run (once Jenkins is healthy):

```
$ oc start-build widget-jenkins-agent-pipeline
```

Your password to Jenkins will be same as your OpenShift password (`admin`:`redhat01`). After this image is built, it
will show up as an available agent in the kubernetes-plugin configuration section in your Jenkins instance and can
be used by specifying the label `widget-jenkins-agent`.

=== 2.3 Review Application

```
$ cd $GIT_BASE/widget-factory
```

Now let's finally take a look at that widget-factory service. It's a simple spring data service, one controller is setup
as a `spring-data-rest` interface that autoconfigures CRUD operations on our `widget` object. There is a second
controller that exposes a service interface tied to a widget repository interface allowing for building more custom
queries. The important parts of the application (for the purpose of this lab) are how we are planning to automate
building, deploying and connecting the application to our database (for now `widgettest` configured in section-1).

=== 2.4 Ansible-Applier

Let's take a look inside the `.applier` folder, under `templates` you'll see a number of YAML files specifying an
OpenShift template for various resources. As you may expect, `build.yml` specifies how to build and store the image,
while `deploy.yml` specifies how to deploy the application. The `db-service.yml` contains configuration for how to
connect to our database, exposing the external hostname of the server as an OpenShift internal service (more
https://docs.openshift.com/container-platform/3.10/dev_guide/integrating_external_services.html[info]). It also creates
the encoded secret `mysql` that our deployment uses.

You will need to update the `Jenkinsfile` with your database hostname:

```
openshift.withCluster() {
    env.NAMESPACE = openshift.project()
    env.APPLICATION_NAME = "widget-factory"
    env.APPLICATION_RELEASE = "0.0.1"
    env.DATABASE_HOST = "ec2-18-234-37-92.compute-1.amazonaws.com" <- This line should match your database host
    echo "Starting Pipeline for ${APPLICATION_NAME}..."
}
```

=== 2.5 Deploy Application

With this in place we are ready to deploy our application. Let's create our application pipeline:

```
$ oc process -f widget-pipeline.yml --param=SOURCE_REF=section-2 | oc apply -f-
$ oc start-build widget-factory-pipeline
```

Now go into Jenkins to watch your build continue.

== 3 Self-service MySQL DB Provisioning

=== 3.1 Automation Service Broker

=== 3.2 Build an APB

```
$ git checkout section-3
$ cd ../database-provision-apb

...

$ oc new-build --binary=true --name database-provision-apb
$ oc policy add-role-to-user system:image-builder system:serviceaccount:widget-factory:builder -n openshift
$ oc patch bc/database-provision-apb -p '{"spec": {"output":{"to": {"namespace": "openshift" } } } }'

...

$ apb bundle prepare
$ oc start-build --follow --from-dir . database-provision-apb
$ apb broker bootstrap
```

=== 3.3 Provision Database and Credentials

=== 3.4 Update Application to Use Bindings